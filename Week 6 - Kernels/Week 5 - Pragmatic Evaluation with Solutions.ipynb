{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b485358",
   "metadata": {},
   "source": [
    "# Pragmatic Evaluation (With Solutions)\n",
    "\n",
    "In this notebook, you will learn how to run a k-flod test, as well as create different metrics based on confusion matrix\n",
    "\n",
    "## Authors\n",
    "- Xiao Fu xiao.fu.20@ucl.ac.uk\n",
    "\n",
    "## Learning Outcomes\n",
    "- **K-Fold Cross Validation:** Testing accuracy for just once doesn't account for the variance in the data and might give misleading results. K-Fold validation randomly selects one of $k$ parts of the data set then tests the accuracy on the same. After required number of iterations, the accuracy is averaged\n",
    "- **Confusion matrix:** Confusion matrix is used only on classification tasks. It describes the following matrix\n",
    "\n",
    "## Source\n",
    "- https://github.com/maykulkarni/Machine-Learning-Notebooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd84d3b",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "1. Run a K-fold test\n",
    "2. Build evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e80450",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516a321",
   "metadata": {},
   "source": [
    "## Importing the Dataset\n",
    "\n",
    "A simple dataset available from https://www.kaggle.com/datasets/rakeshrau/social-network-ads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Social_Network_Ads.csv')\n",
    "X = df.iloc[:, 2:4]\n",
    "y = df.iloc[:, 4]\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a5825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data X\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ad3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking targets y\n",
    "y.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73414206",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation\n",
    "https://machinelearningmastery.com/k-fold-cross-validation/\n",
    "\n",
    "\n",
    "        \n",
    "Cross-validation is a method used to assess machine learning models on a limited data sample.\n",
    "\n",
    "This method involves a key parameter named k, indicating the number of subsets the dataset should be divided into. This is why it's frequently referred to as k-fold cross-validation. When you choose a specific value for k, you can name the procedure accordingly, for instance, if k=10, it's called 10-fold cross-validation.\n",
    "\n",
    "Here's the step-by-step process:\n",
    "\n",
    "1. Shuffle the dataset (Recommended but not included in this notebook).\n",
    "\n",
    "2. Divide the dataset into k subsets.\n",
    "\n",
    "3. For each distinct subset:\n",
    "\n",
    "    Use this subset as the testing data.\n",
    "\n",
    "    Utilize the other subsets as the training data.\n",
    "\n",
    "    Train a model using the training data and test its performance on the testing data.\n",
    "\n",
    "    Keep the performance metric and then discard that particular model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6175c2",
   "metadata": {},
   "source": [
    "## Index split\n",
    "In the first step, finish `k_flod_split_index` to split list `indices` into `k` parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23d7647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_flod_split_index(indices:[], k=10) -> []:\n",
    "    splited_indices = []\n",
    "    \n",
    "    #Insert Your Code Here\n",
    "    size = int(len(indices)/k)\n",
    "    for i in range(k):\n",
    "        splited_indices.append(indices[size*i:size*i+size])\n",
    "    #=====================\n",
    "    \n",
    "    return splited_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99175e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.index\n",
    "k_flod_split_index(X.index,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b153da09",
   "metadata": {},
   "source": [
    "## Run K-flod\n",
    "\n",
    "Next, finish `run_eval` to run a K-flod test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e650216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler\n",
    "X_sca = StandardScaler()\n",
    "X = X_sca.fit_transform(X)\n",
    "\n",
    "def run_eval(X,y,k=10,evaluation=accuracy_score):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    scores = pd.DataFrame(['score'])\n",
    "#     scores = pd.DataFrame({'score':[]}) # For Newer Pandas\n",
    "    splited_indices = k_flod_split_index(range(len(X)),k)\n",
    "    for i in range(len(splited_indices)):\n",
    "        \n",
    "        #create lists for train and test indices\n",
    "        #Insert Your Code Here\n",
    "        train_indices = []\n",
    "        for j in range(len(splited_indices)):\n",
    "            if j != i:\n",
    "                train_indices += splited_indices[j]\n",
    "        test_indices = splited_indices[i]\n",
    "        #=====================\n",
    "\n",
    "        X_train, X_test, y_train, y_test = X[train_indices], X[test_indices], \\\n",
    "                                            y[train_indices], y[test_indices]\n",
    "        \n",
    "        # train a new SVC model.\n",
    "        clf = SVC(kernel='linear', random_state=0).fit(X_train, y_train)\n",
    "        \n",
    "        # test the new model.\n",
    "        #Insert Your Code Here\n",
    "        y_test = y_test.values.tolist()\n",
    "        _score = evaluation(y_test, clf.predict(X_test))\n",
    "        #=====================\n",
    "        \n",
    "        # save the performance.\n",
    "        scores = scores.append({'score':_score},ignore_index = True)\n",
    "#         scores.loc[len(scores)] = _score # For Newer Pandas\n",
    "        correct += _score\n",
    "        total += 1\n",
    "    print(\"Ave. score: {0:.2f}\".format(correct/total))\n",
    "    scores.plot.bar()\n",
    "\n",
    "run_eval(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f33fd7e",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "Confusion matrix is used only on classification tasks. It describes the following matrix\n",
    "\n",
    "|            | predicted true | predicted false |\n",
    "|------------|----------------|-----------------|\n",
    "|actual true | True Positive  | False Negative  |\n",
    "|actual false| False Positive | True Negative   |\n",
    "\n",
    "Complete Class `ConfusionMatrix`: In the function `calculate_matrix` four parts of the matrix created in `__init__` should be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfusionMatrix():\n",
    "    def __init__(self):\n",
    "        self.tp = 0\n",
    "        self.tn = 0\n",
    "        self.fp = 0\n",
    "        self.fn = 0\n",
    "        \n",
    "    def calculate_matrix(self, y_true:[], y_pred:[]):\n",
    "        self.__init__()\n",
    "        #Insert Your Code Here\n",
    "        for i in range(len(y_true)):\n",
    "            if y_true[i] == 1 and y_pred[i] == 1:\n",
    "                self.tp += 1\n",
    "            elif y_true[i] == 1 and y_pred[i] == 0:\n",
    "                self.fn += 1\n",
    "            elif y_true[i] == 0 and y_pred[i] == 1:\n",
    "                self.fp += 1\n",
    "            elif y_true[i] == 0 and y_pred[i] == 0:\n",
    "                self.tn += 1\n",
    "        #====================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef30b3",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "Class `Accuracy` is extended from class `ConfusionMatrix`, which means they share functions.\n",
    "\n",
    "Complete the function `accuracy` using features from class `ConfusionMatrix`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy(ConfusionMatrix):\n",
    "    def __init__(self):\n",
    "        super()\n",
    "    def accuracy(self, y_true:[], y_pred:[]):\n",
    "        self.calculate_matrix(y_pred,y_true)\n",
    "        acc=-1\n",
    "        #Insert Your Code Here\n",
    "        acc = (self.tp+self.tn)/(self.tp+self.tn+self.fp+self.fn)\n",
    "        #=====================\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1936113",
   "metadata": {},
   "source": [
    "Now we can call your accuracy in the K-fold test. Does it get the same result as the previous test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec54a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "_eval = Accuracy()\n",
    "run_eval(X,y,evaluation=_eval.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c1e7a0",
   "metadata": {},
   "source": [
    "## Precision (Positive Predicted Value) \n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "Intuitively, what precision states is out of the number of times your model predicts true, how many times is it correct? This metric penalizes heavily for False Positives. This metric should be considered when its OK to have some false negatives but not false positives. Imagine if your model is predicting the conclusion of a jurisdiction. Its OK to leave a criminal free, rather than punishing an innocent one. \n",
    "\n",
    "## Recall (Sensitivity) \n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Intuitively, what recall states is out of the times the output is true, how many times are you correct? This metric penalizes heavily for False Negatives. This metric should be considered when its OK to have some false positives but not false negatives.\n",
    "\n",
    "\n",
    "## F1 Score\n",
    "\n",
    "F1 score is the harmonic mean of precision and recall. \n",
    "\n",
    "\n",
    "$$\\text{F}_1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$\n",
    "\n",
    "\n",
    "Finish `PrecisionAndRecall` where Precision, Recall and F1 calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfcd59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionAndRecall(ConfusionMatrix):\n",
    "    def __init__(self):\n",
    "        super()\n",
    "        \n",
    "    def precision(self, y_true:[], y_pred:[]):\n",
    "        self.calculate_matrix(y_true,y_pred)\n",
    "        p=-1\n",
    "        #Insert Your Code Here\n",
    "        if not self.tp+self.fp:\n",
    "            p=0\n",
    "        else:\n",
    "            p = (self.tp)/(self.tp+self.fp)\n",
    "        #=====================\n",
    "        return p\n",
    "    \n",
    "    def recall(self, y_true:[], y_pred:[]):\n",
    "        self.calculate_matrix(y_true,y_pred)\n",
    "        r=-1\n",
    "        #Insert Your Code Here\n",
    "        if not self.tp+self.fn:\n",
    "            r = 0\n",
    "        else:\n",
    "            r = (self.tp)/(self.tp+self.fn)\n",
    "        #=====================\n",
    "        return r\n",
    "    \n",
    "    def f1(self, y_true:[], y_pred:[]):\n",
    "        self.calculate_matrix(y_true,y_pred)\n",
    "        f1=-1\n",
    "        #Insert Your Code Here\n",
    "        p = self.precision(y_true,y_pred)\n",
    "        r = self.recall(y_true,y_pred)\n",
    "        if not p+r:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2*(p*r)/(p+r)\n",
    "        #=====================\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b8de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_eval = PrecisionAndRecall()\n",
    "run_eval(X,y,evaluation=_eval.precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df8fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(X,y,evaluation=_eval.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8772a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(X,y,evaluation=_eval.f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59665f9",
   "metadata": {},
   "source": [
    "# Have a try\n",
    "During this lesson, you were introduced to the methodology of executing a K-fold test and the steps to devise a performance metric.\n",
    "\n",
    "When you revisit the code, you'll see that the evaluation procedure encompasses three segments:\n",
    "\n",
    "1. Testing structure (in this notebook: `run_eval`)\n",
    "2. The predictive model (in this notebook: `SVC`)\n",
    "3. Evaluation metrics (in this notebook: `Accuracy` and `PrecisionAndRecall`)\n",
    "\n",
    "When you craft a new framework or metric, you're essentially shaping a fresh evaluation procedure. This procedure ought to be **aligned** with your specific projects or professional requirements.\n",
    "\n",
    "How about a **brand-new** scenario?\n",
    "\n",
    "Or perhaps a topic **unique** to you?\n",
    "\n",
    "Maybe you should think about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44411792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80282760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51df08a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
