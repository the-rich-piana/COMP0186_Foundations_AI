{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b485358",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "This notebook demonstrates how to implement the kmeans clustering algorithm using numpy. The assumptions of the kmeans algorithm are equivalent to assuming that the underlying data generating process is defined by k different gaussian distributions with constant variance but different means i.e., the pdf is assumed to be of the form:\n",
    "$p(x) = \\sum_{i=1}^{k}N(\\mu_{i}, \\sigma)$ were k defines the number of clusters.\n",
    "\n",
    "Kmeans is a non-parametric algorithm and uses distances between points within the metric space defined by the input data. \n",
    "\n",
    "\n",
    "## Authors\n",
    "- Joshua Spear, joshua.spear.21@ucl.ac.uk\n",
    "\n",
    "## Learning Outcomes\n",
    "- Gain intuition regarding how the kmeans algorithm is implemented, in particular how the cluster assignment stage and cluster centre calculation stage iterate\n",
    "    - Students should focus on implementing the algorithm using numpy operations to understand how to efficiently handle vector and matrix structures in Python\n",
    "    - Students should also pay attention to the additional steps required when implementing algorithms on real data. For example, reinitialisation of cluster centres\n",
    "- Gain intuition regarding how the number of clusters can be chosen using the silhouette score\n",
    "\n",
    "Extension:\n",
    "- Gain intuition regarding how different distance metrics can be used to handle different types of data\n",
    "- Gain intuition regarding how the assumptions of kmeans effects performance of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd84d3b",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "In order to implement and understand the kmeans algorithm, data will be simulated using the 'make_blobs' function from sklearn. The notebook will then demonstrate how to implement the kmeans algorithm using object-orientated programming in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e80450",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c90afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special interface functionality in Python (described later)\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from typing import Callable\n",
    "\n",
    "# Popular library in Python for manipulating vector, matrices and larger tensor objects\n",
    "import numpy as np\n",
    "\n",
    "# Required for dataset construction\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Required for extension\n",
    "from fastdtw import fastdtw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516a321",
   "metadata": {},
   "source": [
    "## Generating the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89330003",
   "metadata": {},
   "source": [
    "For the initial task, a dataset will be generated by parameterising isotropic (spherical) gaussian distributions with constant standard deviation. This is done by utilising the 'make_blobs' function from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77cb4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "centres = 5 # Number of Multivariate Gaussian distributions in the underlying data generating process\n",
    "n_features = 10 # Dimension of each individual gaussian distribution\n",
    "X, y = make_blobs(\n",
    "    n_samples=100, # Number of independant samples to generate\n",
    "    n_features=n_features,\n",
    "    centers=centres, \n",
    "    random_state=42 # Seed of random number generator to ensure results are repeatable \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519db22",
   "metadata": {},
   "source": [
    "Next, split the data into training and test where 20\\% of observations are held out for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3f92163",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c8818c",
   "metadata": {},
   "source": [
    "## Distance metrics\n",
    "The kmeans algorithm clusters observations based on their distance to different centres. Therefore, the first step of implementing a kmeans algorithm is to define the distance metric to be used. To begin with, the euclidean distance will be used.\n",
    "\n",
    "#### OOP programming\n",
    "Whilst the aim of this tutorial is not to teach OOP programming, a number of different machine learning packages are implemented using 'stateful' OOP implementations therefore, such an implementation will be used here. A class defines a blueprint for creating objects, where objects have data associated with them and generally have methods which 'do things' to that data.\n",
    "\n",
    "'Metaclasses' and 'abstractmethods' are used to provide an interface for implementing different types of classes which are expected to share some structure that is implemented in the child classes. The DistanceMetric metaclass below will be used to build different distance metrics. The \\_\\_call\\_\\_ dunder method allows the class to be called like a function - see the 'HelloWorld' class as an illustration.\n",
    "\n",
    "The DistanceMetric metaclass is used to:\n",
    "- Implement generic functionality which checks that the inputs to the distance metric calculation is consistant (__Metaclass reason 1__)\n",
    "2. Provides a consistent type to use in the KMeans clusterer class later on (__Metaclass reason 2__)\n",
    "\n",
    "The code has been marked with Metaclass reason 1/Metaclass reason 2 to indicate where the metaclass is useful in the later code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a03da20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "class HelloWorld:\n",
    "    \n",
    "    def __call__(self):\n",
    "        print(\"Hello World!\")\n",
    "        \n",
    "hw = HelloWorld()\n",
    "hw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9f5a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceMetric(metaclass=ABCMeta):\n",
    "    distance: np.array | None = None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def distance_calculation(self, x:np.array, y:np.array)->np.array:\n",
    "        \n",
    "        # distance = np.linalg.norm(x - y)\n",
    "        # self.distance = distance\n",
    "    \n",
    "    def __call__(self, x:np.array, y:np.array)->np.array: #Metaclass reason 1\n",
    "        if not (len(x.shape) == len(x.shape) == 1): \n",
    "            raise Exception(\"Both x and y must be a single dimension\")\n",
    "        return self.distance_calculation(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c657e",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Add a method to the EuclideanDistance class defined below, to calculate the euclidean distance between two vectors. \n",
    "\n",
    "__HINT__: The EuclideanDistance class 'inherits' from the DistanceMetric class i.e., it has all of the functionality defined in the DistanceMetric. Therefore, the only functionality that needs to be added to the EuclideanDistance is the 'distance_calculation' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuclideanDistance(DistanceMetric):\n",
    "    \n",
    "    # def distance_calculation(self, x:np.array, y:np.array)->np.array:\n",
    "    #     distance = np.linalg.norm(x - y)\n",
    "    #     self.distance = distance    \n",
    "    #Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b2467",
   "metadata": {},
   "source": [
    "Once we have defined the EuclideanDistance we need to instantiate a EuclideanDistance object. This object can be used any number of times that is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65229aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed = EuclideanDistance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96985d57",
   "metadata": {},
   "source": [
    "## Kmeans clustering class\n",
    "The kmeans clustering algorithm will be implemented as a class with a fit and predict method - similar to the very familiar sklearn API. Implementing the algorithm using the class defined below will make implementing later parts of the machine learning pipeline easier. For example, the KMeansClusterer below is implemented with an _n\\_clusters_ attribute. Therefore, p different instantiations of the kmeans algorithm can easily be implemented which will make hyperparamerter tuning significantly easier.\n",
    "\n",
    "The following few exercises will focus on implementing the get_random_clusters, assign_centres and fit functions that the KMeansClusterer class calls. These functions have been defined externally to the KMeansClusterer class for the puporses of stepping through this notebook - ordinarily they would be implemented directly into the KMeansClusterer class as methods.\n",
    "\n",
    "Within the KMeansClusterer class, a _predict_ method has already been provided however, note that the method relies on _assign_centres_ which has not yet been implemented.  A _check_input_data_ method has also been provided which is used in the _predict_ and _fit_ methods to provide useful debugging information to the end user and prevent incorrect results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e04000",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "The kmeans algorithm relies on an initial setting for the clusters centres, therefore the first step is to implement the _get_random_clusters_ method which will be used to initialise the cluster centres. The method should take the argument, X, which is an np.array defining the training data and therefore should be of shape (number of observations, number of features). The method should return a numpy array of shape (number of cluster centres, number of features). Finally, the cluster centres will be initialised by drawing samples from an independant uniform random variable for each feature, parameterised by the observed min and max for each feature. Use the cell below to check your code.\n",
    "\n",
    "__HINT__: Use the np.random.uniform function to generate an numpy array of shape (number of cluster centres, number of features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_clusters(X, n_clusters):\n",
    "    #Insert Your Code Here \n",
    "    min_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ac09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = centres\n",
    "cluster_init = get_random_clusters(X_train, n_clusters)\n",
    "print(cluster_init)\n",
    "print(cluster_init.shape)\n",
    "if cluster_init.shape != (n_clusters, n_features):\n",
    "    print(\"The shape of your output isn't right :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f91527",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Both the fit and predict methods need to assign observations to a given array of cluster centres. Therefore, this functionality will be abstracted into the assign_centres method. The first line of the method builds a numpy array of shape (number of observations) and returns this as the output therefore, the body of the code should assign a single indes of self.cluster_centres for each observation in X. self.distance_metric(x,y) should be called for each row in X and each cluster in self.cluster_centres.\n",
    "\n",
    "__HINT__: Try defining an outer loop which iterates through the rows of X and an inner loop that iterates through the clusters in self.cluster_centres for a given row in X\n",
    "\n",
    "__HINT__: np.argmin() can be used to obtain the index of the minimum value in an array. This would be useful to apply to an array of distance values assoicated with a given observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa29b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_centres(X:np.array, cluster_centres:np.array, distance_metric:DistanceMetric):\n",
    "    sorted_points = np.tile(np.nan, X.shape[0])\n",
    "    #Insert Your Code Here\n",
    "    \n",
    "    \n",
    "    return sorted_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2507a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignments = assign_centres(X_train, cluster_init, ed)\n",
    "print(cluster_assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f3eabd",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "The fit method below is required to iterate through the kmeans algorithm and output an array of shape (number of clusters, number of features) containing the final cluster assignments.\n",
    "\n",
    "Some functionality has already been provided, including the while loop and functionality to handle instances where cluster centres with 0 observations occur. Additional code should be included in the while loop to:\n",
    "1. Assign observations to clusters and;\n",
    "2. Define the new cluster centres based on these assignments\n",
    "\n",
    "__HINT:__ The assign_centres function from above should be used.\n",
    "\n",
    "__HINT:__ To update the cluster centres, try looping through them and updating the mean values in similar way to how asignment is handled in the line 'self.cluster_centres[na_centres,:] = fb_cluster_centres[na_centres,:]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b403b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClusterer:\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        n_clusters:int, \n",
    "        distance_metric:DistanceMetric, # Metaclass reason 2 \n",
    "        max_iter:300\n",
    "    ):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.distance_metric = distance_metric\n",
    "        self.cluster_centres = None\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    def check_input_data(self, X):\n",
    "        if len(X.shape) != 2:\n",
    "            raise Exception(\"Input data must be two dimensional\")\n",
    "        \n",
    "    def predict(self, X:np.array)->np.array:\n",
    "        self.check_input_data(X)\n",
    "        if self.cluster_centres is None:\n",
    "            raise Exception(\"Fit clusterer before calling predict\")\n",
    "        sorted_points = assign_centres(X, self.cluster_centres, self.distance_metric)\n",
    "        return sorted_points\n",
    "    \n",
    "    def fit(self, X:np.array, seed:int=1)->np.array:\n",
    "        np.random.seed(seed)\n",
    "        self.check_input_data(X)\n",
    "        self.cluster_centres = get_random_clusters(X, self.n_clusters)\n",
    "        #Insert Your Code Here \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmc = KMeansClusterer(n_clusters=n_clusters, distance_metric=ed, max_iter=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9901b562",
   "metadata": {},
   "source": [
    "**TODO**: Clusters collapse when the number of # clusters >> #features. Fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dceedef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmc.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da17c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmc.cluster_centres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e827d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = kmc.predict(X_train) \n",
    "test_pred = kmc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf317978",
   "metadata": {},
   "source": [
    "Plotting the results (with PCA!!!), it can be seen that the kmeans algorithm correctly identifies the 5 clusters, albiet by assigning different index labels (hence the change in colour)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca_train = pca.fit_transform(X_train)\n",
    "pca_test = pca.transform(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(18,9))\n",
    "ax[0].scatter(pca_test[:,0], pca_test[:,1], c=y_test)\n",
    "ax[0].set_ylabel(\"Component 2\")\n",
    "ax[0].set_xlabel(\"Component 1\")\n",
    "ax[1].scatter(pca_test[:,0], pca_test[:,1], c=test_pred)\n",
    "ax[1].set_ylabel(\"Component 2\")\n",
    "ax[1].set_xlabel(\"Component 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f7f41",
   "metadata": {},
   "source": [
    "## Tuning the k hyperparameter\n",
    "\n",
    "Generally speaking when kmeans is used, it is in an unsupervised setting where there is no ground truth to determine model performance. There are a number of unsupervised evaluation metrics however, the silhouette score will be explored here. The silhouette_score is comprised of the average intra cluster distance and the minimum inter cluster distance i.e., it is a measure of how compact clusters are compared to how close together clusters are. Let $a(i)$ and $b(i)$ define the mean intra cluster distances and minimum inter cluster distance for observation $i \\in C_{I}$\n",
    "\\begin{align}\n",
    "    a(i) = \\frac{1}{|C_{I}|-1}\\sum_{j \\in C_{I}, i\\neq j}d(i,j) \\\\\n",
    "    b(i) = \\min_{J\\neq I}\\frac{1}{|C_{J}|}\\sum_{j \\in C_{I}}d(i,j)\n",
    "\\end{align}\n",
    "Then the silhouette value for $i$ is:\n",
    "\\begin{align}\n",
    "    s(i) = \\frac{b(i)-a(i)}{max\\{a(i),b(i)\\}}\n",
    "\\end{align}\n",
    "\n",
    "Generally speaking, the average silhouette score over all samples is used for evaluation. By examining the equation for $s(i)$ one observes that as clusters become further apart i.e., $b(i) \\rightarrow 1$, $s(i) \\rightarrow 1$. Clusters being further apart suggests that there is clearer separation, and therefore better clustering quality. The same result is obtained when $a(i) \\rightarrow 0$ i.e., clusters become more compact and observations within clusters become 'more similar' with respect to the distance metric used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c914cb",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Using: \n",
    "- The silhouette_score metric provided by sklearn (setting the distance to be euclidean) and; \n",
    "- The KMeansClusterer class and ed object from above;\n",
    "Perform a grid search over the number of clusters from 2 to 10 and create a plot of number of clusters against silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72baf59c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sil_score = []\n",
    "clusters = np.arange(2,10)\n",
    "#Insert Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef112f2",
   "metadata": {},
   "source": [
    "## Extension: Kmeans with manhatten distance\n",
    "\n",
    "The manhatten distance is a very popular distance metric in machine learning. Let $x,y \\in \\mathbb{R}^{p}$ then the manhatten distance is defined as:\n",
    "\\begin{align}\n",
    "    \\sum_{i=1}^{p}|x_{i}-y_{i}|\n",
    "\\end{align}\n",
    "\n",
    "It is commonly used in regression as a regularisation criteria (often referred to as the $L_{1}$ distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747a5dc",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Implement the manhetten distance by subclassing the DistanceMetric abstract class. Run kmeans clustering using the manhatten distance metric and visualise the results using the two PCA components from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e1257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert Your Code Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f21f3e3",
   "metadata": {},
   "source": [
    "Experiment with different numbers of clusters and see whether the manhatten distance can reasonably define all of the clusters in the orginal dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5adf5d",
   "metadata": {},
   "source": [
    "## Extension: Altering dataset generating process\n",
    "\n",
    "Ref: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html\n",
    "\n",
    "Try experimenting with the transformation and cluster_std parameters below. They effect the extent to which the anisotropic and unqeual variance datasets, below breach the assumption of kmeans. (After the kernel lecture try thinking about how the transformation parameter might work!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9fb7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "centres = np.array([[2,2],[0,0]])\n",
    "n_features = 2\n",
    "np.random.seed(2)\n",
    "transformation = np.random.uniform(-1,1,(n_features, n_features))\n",
    "cluster_std = [2.5, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(\n",
    "    n_samples=500,\n",
    "    n_features=n_features,\n",
    "    centers=centres,\n",
    "    random_state=42\n",
    ")\n",
    "X_aniso = np.dot(X, transformation)\n",
    "X_varied, y_varied = make_blobs(\n",
    "    n_samples=500, \n",
    "    centers=centres,\n",
    "    cluster_std=cluster_std, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "kmc = KMeansClusterer(n_clusters=2, distance_metric=ed, max_iter=1000)\n",
    "kmc.fit(X)\n",
    "res = kmc.predict(X)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,9))\n",
    "ax[0].scatter(X[:, 0], X[:, 1], c=y)\n",
    "ax[1].scatter(X[:, 0], X[:, 1], c=res)\n",
    "ax[0].set_title(\"Standard\")\n",
    "plt.show()\n",
    "\n",
    "kmc = KMeansClusterer(n_clusters=2, distance_metric=ed, max_iter=1000)\n",
    "kmc.fit(X_aniso)\n",
    "res_aniso = kmc.predict(X_aniso)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,9))\n",
    "ax[0].scatter(X_aniso[:, 0], X_aniso[:, 1], c=y)\n",
    "ax[1].scatter(X_aniso[:, 0], X_aniso[:, 1], c=res_aniso)\n",
    "ax[0].set_title(\"Anisotropically Distributed Blobs\")\n",
    "plt.show()\n",
    "\n",
    "kmc = KMeansClusterer(n_clusters=2, distance_metric=ed, max_iter=1000)\n",
    "kmc.fit(X_varied)\n",
    "res_var = kmc.predict(X_varied)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,9))\n",
    "ax[0].scatter(X_varied[:, 0], X_varied[:, 1], c=y_varied)\n",
    "ax[1].scatter(X_varied[:, 0], X_varied[:, 1], c=res_var)\n",
    "ax[0].set_title(\"Unequal Variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f0ac4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "By the end of this notebook you should have implemented the kmeans clustering algorithm from scratch using numpy as well as experimented with unsupervised hyperparameter tuning using the silhouette score, experimented with different distance metrics and finally gained intuition regarding how the assumptions of kmeans clustering effect the algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
